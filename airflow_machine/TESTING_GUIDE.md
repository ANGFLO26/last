# HÆ°á»›ng dáº«n Test Airflow DAG

## âœ… Giáº£i thÃ­ch vá» cáº£nh bÃ¡o

### 1. SequentialExecutor + SQLite
- **Hiá»‡n táº¡i:** Äang sá»­ dá»¥ng `SequentialExecutor` vá»›i SQLite
- **LÃ½ do:** SQLite khÃ´ng há»— trá»£ concurrent writes tá»‘t, nÃªn chá»‰ tÆ°Æ¡ng thÃ­ch vá»›i SequentialExecutor
- **Cáº£nh bÃ¡o:** ÄÃ¢y lÃ  cáº£nh bÃ¡o thÃ´ng tin, **KHÃ”NG pháº£i lá»—i**
- **PhÃ¹ há»£p:** HoÃ n toÃ n OK cho dev/testing
- **Production:** Náº¿u cáº§n cháº¡y song song nhiá»u tasks, nÃªn:
  - DÃ¹ng PostgreSQL/MySQL + LocalExecutor
  - Hoáº·c CeleryExecutor vá»›i Redis/RabbitMQ

### 2. SQLite Database
- **LÆ°u Ã½:** SQLite váº«n Ä‘Æ°á»£c sá»­ dá»¥ng cho dev/testing
- **Cáº£nh bÃ¡o:** ÄÃ¢y lÃ  cáº£nh bÃ¡o thÃ´ng tin, khÃ´ng pháº£i lá»—i
- **Production:** NÃªn dÃ¹ng PostgreSQL hoáº·c MySQL (cÃ³ thá»ƒ cáº¥u hÃ¬nh sau)

## ğŸ”„ Restart Airflow Ä‘á»ƒ Ã¡p dá»¥ng thay Ä‘á»•i

```bash
cd airflow_machine
source venv/bin/activate
export AIRFLOW_HOME=$(pwd)

# Dá»«ng Airflow
bash stop.sh

# Khá»Ÿi Ä‘á»™ng láº¡i
bash start.sh
```

## ğŸ§ª Test DAG: fraud_detection_pipeline

### BÆ°á»›c 1: Kiá»ƒm tra DAG trong UI

1. Truy cáº­p: `http://192.168.1.50:8080`
2. ÄÄƒng nháº­p: `admin` / `admin`
3. TÃ¬m DAG `fraud_detection_pipeline`
4. Kiá»ƒm tra DAG khÃ´ng cÃ³ lá»—i (khÃ´ng cÃ³ biá»ƒu tÆ°á»£ng cáº£nh bÃ¡o Ä‘á»)

### BÆ°á»›c 2: Kiá»ƒm tra Connections

1. VÃ o **Admin â†’ Connections**
2. TÃ¬m hoáº·c táº¡o connection vá»›i ID: `spark_default`
3. Náº¿u chÆ°a cÃ³, táº¡o má»›i:
   - **Conn Id:** `spark_default`
   - **Conn Type:** `Spark`
   - **Host:** `192.168.1.134`
   - **Port:** `7077`
   - **Extra:** `{"master": "spark://192.168.1.134:7077"}`

### BÆ°á»›c 3: Kiá»ƒm tra Services

**Kiá»ƒm tra Kafka:**
```bash
# Tá»« Airflow machine
telnet 192.168.1.60 9092
# Hoáº·c
nc -zv 192.168.1.60 9092
```

**Kiá»ƒm tra Spark:**
```bash
# Tá»« Airflow machine
curl http://192.168.1.134:8080
# Hoáº·c
curl http://192.168.1.134:7077
```

### BÆ°á»›c 4: KÃ­ch hoáº¡t DAG

1. Trong Airflow UI, tÃ¬m DAG `fraud_detection_pipeline`
2. Toggle switch tá»« OFF â†’ ON (bÃªn trÃ¡i tÃªn DAG)
3. DAG sáº½ chuyá»ƒn sang tráº¡ng thÃ¡i "Active"

### BÆ°á»›c 5: Trigger DAG Run

**CÃ¡ch 1: Tá»« UI**
1. Click vÃ o tÃªn DAG `fraud_detection_pipeline`
2. Click nÃºt **"Play"** (â–¶ï¸) á»Ÿ gÃ³c trÃªn bÃªn pháº£i
3. Chá»n **"Trigger DAG"**
4. XÃ¡c nháº­n trigger

**CÃ¡ch 2: Tá»« Command Line**
```bash
cd airflow_machine
source venv/bin/activate
export AIRFLOW_HOME=$(pwd)

# Trigger DAG
airflow dags trigger fraud_detection_pipeline
```

### BÆ°á»›c 6: Theo dÃµi DAG Run

1. **Trong UI:**
   - Click vÃ o DAG name
   - Xem Graph View Ä‘á»ƒ tháº¥y flow cá»§a tasks
   - Xem Tree View Ä‘á»ƒ tháº¥y lá»‹ch sá»­ runs
   - Click vÃ o task Ä‘á»ƒ xem logs

2. **Tá»« Command Line:**
```bash
# Xem danh sÃ¡ch DAG runs
airflow dags list-runs -d fraud_detection_pipeline

# Xem task instances
airflow tasks list fraud_detection_pipeline

# Xem logs cá»§a má»™t task
airflow tasks logs fraud_detection_pipeline <task_id> <execution_date>
```

### BÆ°á»›c 7: Kiá»ƒm tra tá»«ng Task

DAG `fraud_detection_pipeline` cÃ³ cÃ¡c tasks theo thá»© tá»±:

1. **verify_kafka_ready** - Kiá»ƒm tra Kafka accessible
2. **verify_spark_ready** - Kiá»ƒm tra Spark Master accessible
3. **verify_data_files** - Kiá»ƒm tra data files tá»“n táº¡i
4. **train_model** - Train ML model vá»›i Spark
5. **start_spark_streaming** - Start Spark streaming job
6. **verify_streaming_running** - Verify streaming job Ä‘ang cháº¡y
7. **start_producer** - Start Kafka producer
8. **start_viewer** - Start Streamlit viewer

**Kiá»ƒm tra logs:**
- Click vÃ o tá»«ng task trong Graph View
- Xem logs Ä‘á»ƒ kiá»ƒm tra output
- Kiá»ƒm tra status (success/failed)

## ğŸ” Troubleshooting

### Task bá»‹ failed

1. **Xem logs:**
   - Click vÃ o task failed
   - Xem tab "Log" Ä‘á»ƒ biáº¿t lá»—i

2. **CÃ¡c lá»—i thÆ°á»ng gáº·p:**

   **Lá»—i: "Kafka is not accessible"**
   - Kiá»ƒm tra Kafka Ä‘ang cháº¡y: `telnet 192.168.1.60 9092`
   - Kiá»ƒm tra firewall: `sudo ufw status`
   - Kiá»ƒm tra IP Ä‘Ãºng trong DAG

   **Lá»—i: "Spark Master is not accessible"**
   - Kiá»ƒm tra Spark Ä‘ang cháº¡y: `curl http://192.168.1.134:8080`
   - Kiá»ƒm tra connection `spark_default` trong Airflow
   - Kiá»ƒm tra IP Ä‘Ãºng trong DAG

   **Lá»—i: "Data files not found"**
   - Kiá»ƒm tra file tá»“n táº¡i: `ls -lh data/train.csv data/stream.csv`
   - Kiá»ƒm tra Ä‘Æ°á»ng dáº«n trong DAG Ä‘Ãºng

   **Lá»—i: "Connection refused" trong SparkSubmitOperator**
   - Kiá»ƒm tra Spark connection trong Airflow UI
   - Kiá»ƒm tra Spark Master Ä‘ang cháº¡y
   - Kiá»ƒm tra network connectivity

### DAG khÃ´ng cháº¡y

1. **Kiá»ƒm tra DAG active:**
   - Toggle switch pháº£i ON (mÃ u xanh)

2. **Kiá»ƒm tra Scheduler:**
   ```bash
   # Xem scheduler logs
   tail -f logs/scheduler.log
   
   # Kiá»ƒm tra scheduler Ä‘ang cháº¡y
   ps aux | grep "airflow scheduler"
   ```

3. **Kiá»ƒm tra DAG syntax:**
   ```bash
   cd airflow_machine
   source venv/bin/activate
   export AIRFLOW_HOME=$(pwd)
   
   # List DAGs
   airflow dags list
   
   # Kiá»ƒm tra DAG cá»¥ thá»ƒ
   airflow dags show fraud_detection_pipeline
   ```

## ğŸ“Š Kiá»ƒm tra káº¿t quáº£

### Sau khi DAG cháº¡y thÃ nh cÃ´ng:

1. **Model Ä‘Æ°á»£c train:**
   - Kiá»ƒm tra model file trÃªn Spark machine: `ls -lh /models/fraud_detection_v1/`

2. **Streaming job Ä‘ang cháº¡y:**
   - Kiá»ƒm tra Spark UI: `http://192.168.1.134:8080`
   - TÃ¬m streaming job trong "Running Applications"

3. **Kafka messages:**
   - Kiá»ƒm tra Kafka topics: `kafka-topics.sh --list --bootstrap-server 192.168.1.60:9092`
   - Kiá»ƒm tra messages: `kafka-console-consumer.sh --bootstrap-server 192.168.1.60:9092 --topic prediction_output --from-beginning`

4. **Streamlit Viewer:**
   - Truy cáº­p: `http://192.168.1.50:8501`
   - Xem real-time predictions

## ğŸ¯ Quick Test Commands

```bash
# 1. KÃ­ch hoáº¡t venv
cd airflow_machine
source venv/bin/activate
export AIRFLOW_HOME=$(pwd)

# 2. Kiá»ƒm tra DAG
airflow dags list | grep fraud_detection

# 3. Trigger DAG
airflow dags trigger fraud_detection_pipeline

# 4. Xem DAG runs
airflow dags list-runs -d fraud_detection_pipeline --state running

# 5. Xem logs
tail -f logs/scheduler.log
```

## âœ… Checklist Test

- [ ] Airflow UI accessible
- [ ] DAG `fraud_detection_pipeline` hiá»ƒn thá»‹
- [ ] DAG khÃ´ng cÃ³ lá»—i (no broken DAG)
- [ ] Connection `spark_default` Ä‘Ã£ Ä‘Æ°á»£c táº¡o
- [ ] Kafka accessible (192.168.1.60:9092)
- [ ] Spark Master accessible (192.168.1.134:7077)
- [ ] Data files tá»“n táº¡i
- [ ] DAG Ä‘Æ°á»£c toggle ON
- [ ] DAG run Ä‘Æ°á»£c trigger thÃ nh cÃ´ng
- [ ] Táº¥t cáº£ tasks cháº¡y thÃ nh cÃ´ng
- [ ] Model Ä‘Æ°á»£c train
- [ ] Streaming job Ä‘ang cháº¡y
- [ ] Streamlit viewer accessible
