#!/bin/bash

# Script để setup deployment trên máy mới
# Tự động detect paths và IPs, update vào DAG

echo "=========================================="
echo "Fraud Detection Pipeline - Deployment Setup"
echo "=========================================="

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$SCRIPT_DIR"

# Detect project structure
DAG_DIR="$SCRIPT_DIR/dags"
SCRIPTS_DIR="$SCRIPT_DIR/scripts"
UTILS_DIR="$SCRIPT_DIR/utils"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
DATA_DIR="$PROJECT_ROOT/data"

echo ""
echo "Detected paths:"
echo "  DAG_DIR: $DAG_DIR"
echo "  SCRIPTS_DIR: $SCRIPTS_DIR"
echo "  UTILS_DIR: $UTILS_DIR"
echo "  DATA_DIR: $DATA_DIR"
echo ""

# Get IP addresses
echo "Please provide IP addresses for services:"
read -p "Kafka Machine IP [default: 192.168.1.60]: " KAFKA_IP
KAFKA_IP=${KAFKA_IP:-192.168.1.60}

read -p "Spark Machine IP [default: 192.168.1.134]: " SPARK_IP
SPARK_IP=${SPARK_IP:-192.168.1.134}

read -p "Kafka Port [default: 9092]: " KAFKA_PORT
KAFKA_PORT=${KAFKA_PORT:-9092}

read -p "Spark Master Port [default: 7077]: " SPARK_MASTER_PORT
SPARK_MASTER_PORT=${SPARK_MASTER_PORT:-7077}

read -p "Spark Web UI Port [default: 8080]: " SPARK_WEB_UI_PORT
SPARK_WEB_UI_PORT=${SPARK_WEB_UI_PORT:-8080}

echo ""
echo "Spark Machine Configuration:"
read -p "Spark Data Directory [default: /tmp/fraud_data]: " SPARK_DATA_DIR
SPARK_DATA_DIR=${SPARK_DATA_DIR:-/tmp/fraud_data}

read -p "Spark Models Directory [default: /tmp/fraud_models]: " SPARK_MODELS_DIR
SPARK_MODELS_DIR=${SPARK_MODELS_DIR:-/tmp/fraud_models}

read -p "Spark Checkpoints Directory [default: /checkpoints]: " SPARK_CHECKPOINTS_DIR
SPARK_CHECKPOINTS_DIR=${SPARK_CHECKPOINTS_DIR:-/checkpoints}

echo ""
echo "=========================================="
echo "Configuration Summary:"
echo "=========================================="
echo "Kafka: $KAFKA_IP:$KAFKA_PORT"
echo "Spark Master: $SPARK_IP:$SPARK_MASTER_PORT"
echo "Spark Web UI: $SPARK_IP:$SPARK_WEB_UI_PORT"
echo "Spark Data Dir: $SPARK_DATA_DIR"
echo "Spark Models Dir: $SPARK_MODELS_DIR"
echo "Spark Checkpoints Dir: $SPARK_CHECKPOINTS_DIR"
echo ""

read -p "Continue with this configuration? (y/n): " CONFIRM
if [ "$CONFIRM" != "y" ] && [ "$CONFIRM" != "Y" ]; then
    echo "Setup cancelled."
    exit 0
fi

# Create .env file for Airflow
ENV_FILE="$SCRIPT_DIR/.env"
cat > "$ENV_FILE" << EOF
# Fraud Detection Pipeline Configuration
# Generated by setup_deployment.sh

# IP Addresses
export KAFKA_IP=$KAFKA_IP
export KAFKA_PORT=$KAFKA_PORT
export SPARK_IP=$SPARK_IP
export SPARK_MASTER_PORT=$SPARK_MASTER_PORT
export SPARK_WEB_UI_PORT=$SPARK_WEB_UI_PORT

# Paths
export FRAUD_SCRIPTS_DIR=$SCRIPTS_DIR
export FRAUD_UTILS_DIR=$UTILS_DIR
export FRAUD_DATA_DIR=$DATA_DIR

# Spark Paths (on Spark machine)
export SPARK_DATA_DIR=$SPARK_DATA_DIR
export SPARK_MODELS_DIR=$SPARK_MODELS_DIR
export SPARK_CHECKPOINTS_DIR=$SPARK_CHECKPOINTS_DIR

# Kafka Topics
export KAFKA_INPUT_TOPIC=input_stream
export KAFKA_OUTPUT_TOPIC=prediction_output
EOF

echo ""
echo "✓ Created .env file at $ENV_FILE"
echo ""
echo "To use these environment variables, add this to your ~/.bashrc or ~/.profile:"
echo "  source $ENV_FILE"
echo ""
echo "Or set them in Airflow UI → Admin → Variables"
echo ""

# Verify scripts exist
echo "Verifying scripts..."
MISSING=0

for script in "$SCRIPTS_DIR/train_model.py" "$SCRIPTS_DIR/streaming_inference.py" \
              "$SCRIPTS_DIR/verify_streaming_job.py" "$SCRIPTS_DIR/producer.py" \
              "$SCRIPTS_DIR/viewer.py"; do
    if [ -f "$script" ]; then
        echo "  ✓ $(basename "$script")"
    else
        echo "  ✗ $(basename "$script") - NOT FOUND"
        MISSING=$((MISSING + 1))
    fi
done

if [ -f "$UTILS_DIR/spark_utils.py" ]; then
    echo "  ✓ spark_utils.py"
else
    echo "  ✗ spark_utils.py - NOT FOUND"
    MISSING=$((MISSING + 1))
fi

if [ $MISSING -gt 0 ]; then
    echo ""
    echo "⚠ Warning: $MISSING script(s) not found!"
else
    echo ""
    echo "✓ All scripts found!"
fi

# Verify data files
echo ""
echo "Verifying data files..."
if [ -f "$DATA_DIR/stream.csv" ]; then
    echo "  ✓ stream.csv found"
else
    echo "  ✗ stream.csv NOT FOUND at $DATA_DIR/stream.csv"
fi

echo ""
echo "=========================================="
echo "Setup Complete!"
echo "=========================================="
echo ""
echo "Next steps:"
echo "1. Source environment variables:"
echo "   source $ENV_FILE"
echo ""
echo "2. Verify Spark connection:"
echo "   spark-submit --master spark://$SPARK_IP:$SPARK_MASTER_PORT --version"
echo ""
echo "3. Start Airflow:"
echo "   bash start.sh"
echo ""
echo "4. Configure Airflow Connection:"
echo "   - Go to Airflow UI → Admin → Connections"
echo "   - Edit 'spark_default' connection"
echo "   - Host: $SPARK_IP"
echo "   - Port: $SPARK_MASTER_PORT"
echo "   - Extra: {\"master\": \"spark://$SPARK_IP:$SPARK_MASTER_PORT\"}"
echo ""

