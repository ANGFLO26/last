# FRAUD DETECTION PIPELINE - END-TO-END PROJECT

## ğŸ“‹ Tá»•ng quan

Dá»± Ã¡n end-to-end vá» **Credit Card Fraud Detection** sá»­ dá»¥ng:
- **Spark ML**: Huáº¥n luyá»‡n model tá»« dá»¯ liá»‡u batch
- **Kafka**: Message broker cho streaming data
- **Spark Structured Streaming**: Real-time inference
- **Airflow**: Orchestration toÃ n bá»™ pipeline
- **Streamlit**: Real-time visualization

## ğŸ¯ YÃªu cáº§u Dá»± Ã¡n

1. âœ… Chá»n dataset: Credit Card Fraud Detection (Kaggle)
2. âœ… Chia dá»¯ liá»‡u: `train.csv` (199,365 records) vÃ  `stream.csv` (85,444 records)
3. âœ… Spark ML Training: RandomForest/GBTClassifier
4. âœ… Streaming Pipeline: Producer â†’ Kafka â†’ Spark â†’ Kafka â†’ Viewer
5. âœ… Airflow Orchestration: Äiá»u khiá»ƒn toÃ n bá»™ pipeline

## ğŸ—ï¸ Kiáº¿n trÃºc - 3 MÃY RIÃŠNG BIá»†T

Há»‡ thá»‘ng gá»“m **3 mÃ¡y Ä‘á»™c láº­p**, má»—i mÃ¡y tá»± quáº£n lÃ½ services:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MÃY 1: KAFKA   â”‚    â”‚  MÃY 2: SPARK   â”‚    â”‚ MÃY 3: AIRFLOW  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ - Kafka Broker  â”‚    â”‚ - Spark Master  â”‚    â”‚ - Airflow       â”‚
â”‚ - Zookeeper     â”‚    â”‚ - Spark Workers â”‚    â”‚ - Producer      â”‚
â”‚ - Kafka UI      â”‚    â”‚ - Models Store  â”‚    â”‚ - Viewer        â”‚
â”‚                 â”‚    â”‚ - Checkpoints   â”‚    â”‚ - Scripts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                        Network Communication
```

**LÆ¯U Ã**: KhÃ´ng sá»­ dá»¥ng SSH. Má»—i mÃ¡y tá»± start services vÃ  communicate qua network.

## ğŸ“ Cáº¥u trÃºc Dá»± Ã¡n

```
the_end/
â”œâ”€â”€ data/                          # Data files
â”‚   â”œâ”€â”€ train.csv                 # Training data (199,365 records)
â”‚   â””â”€â”€ stream.csv                 # Streaming data (85,444 records)
â”‚
â”œâ”€â”€ kafka_machine/                 # MÃ¡y 1: Kafka (Ä‘á»™c láº­p)
â”‚   â”œâ”€â”€ docker-compose.yml        # Docker Compose config
â”‚   â”œâ”€â”€ create_topics.sh          # Script táº¡o Kafka topics
â”‚   â”œâ”€â”€ start.sh                  # Start Kafka (cháº¡y trÃªn mÃ¡y nÃ y)
â”‚   â””â”€â”€ stop.sh                   # Stop Kafka
â”‚
â”œâ”€â”€ spark_machine/                 # MÃ¡y 2: Spark (Ä‘á»™c láº­p)
â”‚   â”œâ”€â”€ start_spark_cluster.sh    # Start Spark Master & Workers
â”‚   â”œâ”€â”€ stop_spark_cluster.sh     # Stop Spark
â”‚   â”œâ”€â”€ verify_spark_cluster.sh   # Verify Spark ready
â”‚   â”œâ”€â”€ start.sh                  # Start script (cháº¡y trÃªn mÃ¡y nÃ y)
â”‚   â”œâ”€â”€ stop.sh                   # Stop script
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ spark-defaults.conf   # Spark configuration
â”‚
â”œâ”€â”€ airflow_machine/               # MÃ¡y 3: Airflow (Ä‘á»™c láº­p)
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ fraud_detection_pipeline.py  # Airflow DAG chÃ­nh
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ train_model.py        # Spark ML training script
â”‚   â”‚   â”œâ”€â”€ streaming_inference.py # Spark streaming script
â”‚   â”‚   â”œâ”€â”€ producer.py           # Kafka producer
â”‚   â”‚   â”œâ”€â”€ viewer.py             # Streamlit viewer
â”‚   â”‚   â””â”€â”€ verify_streaming_job.py # Verify Spark job
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ spark_utils.py        # Spark utility functions
â”‚   â”œâ”€â”€ start.sh                  # Start Airflow (cháº¡y trÃªn mÃ¡y nÃ y)
â”‚   â”œâ”€â”€ stop.sh                   # Stop Airflow
â”‚   â””â”€â”€ requirements.txt          # Python dependencies
â”‚
â”œâ”€â”€ .gitignore                     # Git ignore file
â”œâ”€â”€ request.txt                    # YÃªu cáº§u dá»± Ã¡n gá»‘c
â””â”€â”€ README.md                      # File nÃ y
```

## ğŸš€ Quick Start

### BÆ°á»›c 1: Clone Repository

TrÃªn **má»—i mÃ¡y**, clone repository:

```bash
git clone <your-repo-url>
cd the_end
```

### BÆ°á»›c 2: Deploy Kafka Machine

**TrÃªn Kafka Machine:**

```bash
cd kafka_machine

# CÃ i Docker náº¿u chÆ°a cÃ³
sudo apt-get update
sudo apt-get install docker.io docker-compose
sudo usermod -aG docker $USER
# Logout vÃ  login láº¡i

# Start Kafka
chmod +x start.sh
./start.sh
```

**Script sáº½ tá»± Ä‘á»™ng:**
- Detect IP cá»§a mÃ¡y
- Update `docker-compose.yml` vá»›i IP thá»±c táº¿
- Start Kafka, Zookeeper, Kafka UI
- Táº¡o topics: `input_stream`, `prediction_output`

**Verify:**
```bash
# Check containers
docker ps

# Check Kafka UI
# Má»Ÿ browser: http://kafka-machine-ip:8080
```

### BÆ°á»›c 3: Deploy Spark Machine

**TrÃªn Spark Machine:**

```bash
cd spark_machine

# CÃ i Ä‘áº·t Spark (náº¿u chÆ°a cÃ³)
wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
tar -xzf spark-3.5.0-bin-hadoop3.tgz
sudo mv spark-3.5.0-bin-hadoop3 /opt/spark

# Set environment
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
echo 'export SPARK_HOME=/opt/spark' >> ~/.bashrc
echo 'export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin' >> ~/.bashrc

# Start Spark
chmod +x start.sh
./start.sh
```

**Script sáº½ tá»± Ä‘á»™ng:**
- Detect IP cá»§a mÃ¡y
- Update configs vá»›i IP thá»±c táº¿
- Copy configs vÃ o `$SPARK_HOME/conf/`
- Táº¡o thÆ° má»¥c `/models` vÃ  `/checkpoints`
- Start Spark Master vÃ  Workers

**Verify:**
```bash
# Check Spark Web UI
# Má»Ÿ browser: http://spark-machine-ip:8080

# Test connection
spark-submit --master spark://spark-machine-ip:7077 --version
```

### BÆ°á»›c 4: Deploy Airflow Machine

**TrÃªn Airflow Machine:**

```bash
cd airflow_machine

# CÃ i dependencies
pip install -r requirements.txt

# âš ï¸ QUAN TRá»ŒNG: CÃ i Spark Client (SparkSubmitOperator cáº§n spark-submit)
chmod +x install_spark_client.sh
./install_spark_client.sh

# Sau khi cÃ i, thÃªm vÃ o ~/.bashrc hoáº·c ~/.profile:
# export SPARK_HOME=/opt/spark
# export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
# source ~/.bashrc

# Verify Spark client
spark-submit --master spark://192.168.1.134:7077 --version

# Start Airflow
chmod +x start.sh
./start.sh
```

**Script sáº½:**
- Initialize Airflow database (láº§n Ä‘áº§u)
- Táº¡o admin user: `admin/admin`
- Há»i IP cá»§a Kafka vÃ  Spark machines
- Update IPs trong DAG vÃ  scripts
- Start Airflow Scheduler vÃ  Webserver

**Verify:**
```bash
# Má»Ÿ browser: http://airflow-machine-ip:8080
# Login: admin/admin
```

### BÆ°á»›c 5: Config Airflow Connection

1. Má»Ÿ Airflow UI: `http://airflow-machine-ip:8080`
2. Login: `admin/admin`
3. **Admin â†’ Connections â†’ Add:**
   - **Conn Id**: `spark_default`
   - **Conn Type**: `Spark`
   - **Host**: `spark-machine-ip`
   - **Port**: `7077`
   - **Extra**: `{"master": "spark://spark-machine-ip:7077"}`

### BÆ°á»›c 6: Run Pipeline

1. Trong Airflow UI, tÃ¬m DAG `fraud_detection_pipeline`
2. Click **"Trigger DAG"**
3. Monitor tasks trong Graph View
4. Check logs náº¿u cÃ³ lá»—i

## ğŸ”„ Pipeline Flow

```
1. verify_kafka_ready      â†’ Check Kafka accessible (port 9092)
2. verify_spark_ready       â†’ Check Spark accessible (port 7077, 8080)
3. verify_data_files        â†’ Check train.csv vÃ  stream.csv exist
4. train_model              â†’ Train Spark ML model (RandomForest/GBT)
5. start_spark_streaming    â†’ Start Spark streaming job (long-running)
6. verify_streaming_running â†’ Verify Spark streaming job RUNNING
7. start_producer           â†’ Send data tá»« stream.csv vÃ o Kafka
8. start_viewer             â†’ Start Streamlit dashboard
```

**Data Flow:**
```
Producer â†’ Kafka (input_stream) â†’ Spark Streaming â†’ Kafka (prediction_output) â†’ Viewer
```

## ğŸ“ Chi tiáº¿t Scripts

### 1. train_model.py
**Má»¥c Ä‘Ã­ch**: Huáº¥n luyá»‡n Spark ML model tá»« `train.csv`

**Usage:**
```bash
spark-submit train_model.py \
    --input /path/to/train.csv \
    --output /models/fraud_detection_v1 \
    --model-type random_forest
```

**Output:**
- Model táº¡i `/models/fraud_detection_v1/`
- Metrics táº¡i `/models/fraud_detection_v1/metrics.json`

**Features:**
- Preprocessing: Handle missing values, feature scaling
- Models: RandomForest hoáº·c GBTClassifier
- Evaluation: AUC, Accuracy, Precision, Recall, F1

### 2. streaming_inference.py
**Má»¥c Ä‘Ã­ch**: Spark Structured Streaming Ä‘á»ƒ predict real-time tá»« Kafka

**Usage:**
```bash
spark-submit streaming_inference.py \
    --model-path /models/fraud_detection_v1 \
    --kafka-bootstrap kafka-machine-ip:9092 \
    --input-topic input_stream \
    --output-topic prediction_output
```

**Features:**
- Read tá»« Kafka topic `input_stream`
- Load trained model
- Predict vÃ  write vÃ o Kafka topic `prediction_output`
- Checkpointing cho fault tolerance

### 3. producer.py
**Má»¥c Ä‘Ã­ch**: Gá»­i dá»¯ liá»‡u tá»« `stream.csv` vÃ o Kafka

**Usage:**
```bash
python3 producer.py \
    --input /path/to/stream.csv \
    --kafka-bootstrap kafka-machine-ip:9092 \
    --topic input_stream \
    --rate 10
```

**Features:**
- Rate limiting: configurable messages/second
- Loop mode: `--loop` Ä‘á»ƒ gá»­i liÃªn tá»¥c
- Error handling vÃ  retry logic

### 4. viewer.py
**Má»¥c Ä‘Ã­ch**: Streamlit app Ä‘á»ƒ visualization real-time

**Usage:**
```bash
streamlit run viewer.py --server.port 8501
```

**Features:**
- Real-time consumption tá»« Kafka
- Dashboard vá»›i metrics vÃ  charts
- Download predictions as CSV

### 5. verify_streaming_job.py
**Má»¥c Ä‘Ã­ch**: Verify Spark streaming job Ä‘Ã£ RUNNING

**Usage:**
```bash
python3 verify_streaming_job.py \
    --spark-ui-url http://spark-machine-ip:8080 \
    --job-name streaming \
    --timeout 600
```

## ğŸ”§ Configuration

### IP Addresses

**Tá»± Ä‘á»™ng detect:**
- Kafka Machine: `start.sh` tá»± Ä‘á»™ng detect vÃ  update IP
- Spark Machine: `start.sh` tá»± Ä‘á»™ng detect vÃ  update IP
- Airflow Machine: `start.sh` sáº½ há»i IP cá»§a Kafka vÃ  Spark machines

**Manual config (náº¿u cáº§n):**
- Sá»­a IP trong `dags/fraud_detection_pipeline.py`
- Sá»­a IP trong cÃ¡c scripts náº¿u cáº§n

### Network Requirements

**Kafka Machine:**
- Port 9092: Kafka Broker
- Port 2181: Zookeeper
- Port 8080: Kafka UI

**Spark Machine:**
- Port 7077: Spark Master
- Port 8080: Spark Web UI

**Airflow Machine:**
- Port 8080: Airflow Webserver
- Port 8501: Streamlit Viewer

**Firewall:**
```bash
# TrÃªn má»—i mÃ¡y, má»Ÿ ports
sudo ufw allow 9092/tcp
sudo ufw allow 2181/tcp
sudo ufw allow 7077/tcp
sudo ufw allow 8080/tcp
sudo ufw allow 8501/tcp
```

### Airflow Connections

Config trong Airflow UI â†’ Admin â†’ Connections:

**Spark Connection:**
- Conn Id: `spark_default`
- Conn Type: `Spark`
- Host: `spark-machine-ip`
- Port: `7077`
- Extra: `{"master": "spark://spark-machine-ip:7077"}`

## ğŸ“¦ Requirements

### Kafka Machine
- **OS**: Linux (Ubuntu/Debian recommended)
- **Docker**: 20.10+
- **Docker Compose**: 1.29+
- **RAM**: 2GB+
- **Disk**: 10GB+

### Spark Machine
- **OS**: Linux (Ubuntu/Debian recommended)
- **Java**: 8+
- **Spark**: 3.5.0+
- **RAM**: 4GB+ (8GB recommended)
- **Disk**: 20GB+ (cho models vÃ  checkpoints)

### Airflow Machine
- **OS**: Linux (Ubuntu/Debian recommended)
- **Python**: 3.8+
- **RAM**: 4GB+
- **Disk**: 10GB+

**Python Packages:**
```bash
pip install -r airflow_machine/requirements.txt
```

## ğŸ› Troubleshooting

### Kafka khÃ´ng start
```bash
# Check Docker
docker ps
docker logs kafka

# Check ports
netstat -tulpn | grep 9092

# Restart
cd kafka_machine
docker-compose restart
```

### Spark khÃ´ng start
```bash
# Check Spark processes
jps | grep -E "Master|Worker"

# Check logs
tail -f $SPARK_HOME/logs/spark-*-master-*.out

# Check Web UI
curl http://spark-machine-ip:8080

# Restart
cd spark_machine
./stop.sh
./start.sh
```

### Airflow DAG khÃ´ng cháº¡y
```bash
# Check DAG syntax
airflow dags list

# Check connections
# Airflow UI â†’ Admin â†’ Connections

# Check logs
airflow tasks logs fraud_detection_pipeline <task_id> <execution_date>

# Check Spark connection
spark-submit --master spark://spark-machine-ip:7077 --version
```

### Network connectivity issues
```bash
# Test tá»« Airflow machine
telnet kafka-machine-ip 9092
telnet spark-machine-ip 7077

# Test tá»« Spark machine
telnet kafka-machine-ip 9092

# Check firewall
sudo ufw status
```

### Model khÃ´ng load Ä‘Æ°á»£c
```bash
# Check model path exists trÃªn Spark machine
ls -lh /models/fraud_detection_v1/

# Check permissions
sudo chmod -R 777 /models
```

## ğŸ“Š Dataset

- **Dataset**: Credit Card Fraud Detection
- **Source**: Kaggle
- **Problem**: Binary Classification (Fraud Detection)
- **Features**: 
  - Time, V1-V28 (PCA features), Amount
  - Class (0=Normal, 1=Fraud)
- **Train**: 199,365 records (199,030 normal, 334 fraud)
- **Stream**: 85,444 records (85,285 normal, 158 fraud)
- **Imbalanced**: ~0.17% fraud rate

## ğŸ¯ Features

- âœ… **KhÃ´ng cáº§n SSH**: Má»—i mÃ¡y tá»± quáº£n lÃ½
- âœ… **Tá»± Ä‘á»™ng config IP**: Scripts tá»± detect vÃ  update
- âœ… **Spark ML Training**: RandomForest/GBTClassifier vá»›i evaluation metrics
- âœ… **Real-time Streaming**: Spark Structured Streaming vá»›i checkpointing
- âœ… **Kafka Integration**: Topics cho input vÃ  output
- âœ… **Airflow Orchestration**: DAG vá»›i dependencies vÃ  retry logic
- âœ… **Real-time Visualization**: Streamlit dashboard vá»›i charts
- âœ… **Fault Tolerance**: Checkpointing cho streaming jobs
- âœ… **Error Handling**: Retry logic vÃ  error reporting

## ğŸ“ Notes

1. **Má»—i mÃ¡y Ä‘á»™c láº­p**: Clone repository vÃ  cháº¡y `start.sh` trÃªn tá»«ng mÃ¡y
2. **KhÃ´ng cáº§n SSH**: Communication qua network (Kafka, Spark Master)
3. **IP Auto-detect**: Scripts tá»± Ä‘á»™ng detect vÃ  config IP
4. **GitHub Ready**: Cáº¥u trÃºc sáºµn sÃ ng Ä‘á»ƒ push lÃªn GitHub
5. **Data Files**: `data/` folder cÃ³ thá»ƒ commit hoáº·c dÃ¹ng Git LFS cho files lá»›n

## âœ… KIá»‚M TRA Há»† THá»NG

### ÄÃ£ kiá»ƒm tra:
- âœ… **Python syntax**: Táº¥t cáº£ scripts khÃ´ng cÃ³ lá»—i syntax
- âœ… **Shell syntax**: Táº¥t cáº£ scripts khÃ´ng cÃ³ lá»—i syntax
- âœ… **DAG syntax**: ÄÃºng Airflow 2.x format
- âœ… **Paths**: ÄÃ£ sá»­a Ä‘á»ƒ flexible (há»— trá»£ multiple locations)
- âœ… **IP Configuration**: Auto-detect vÃ  update
- âœ… **Dependencies**: Äáº§y Ä‘á»§ trong requirements.txt
- âœ… **No SSH**: KhÃ´ng sá»­ dá»¥ng SSHOperator (Ä‘Ã£ loáº¡i bá»)

### Váº¥n Ä‘á» Ä‘Ã£ sá»­a:
1. âœ… DAG paths: `{{ dag.folder }}` â†’ `{{ dag.dag_folder }}` (Airflow 2.x)
2. âœ… Data paths: Há»— trá»£ multiple locations (tÃ¬m á»Ÿ 3 nÆ¡i)
3. âœ… Kafka topics script: Há»— trá»£ Docker container
4. âœ… Airflow logs: Táº¡o folder trÆ°á»›c khi ghi
5. âœ… IP updates: Update trong cáº£ scripts vÃ  utils
6. âœ… Spark config: Há»i vÃ  update Kafka IP

### Checklist trÆ°á»›c khi deploy:

**Kafka Machine:**
- [ ] Docker & Docker Compose installed
- [ ] Ports 9092, 2181, 8080 opened
- [ ] Run `./start.sh`
- [ ] Verify: `docker ps | grep kafka`
- [ ] Verify topics: `docker exec kafka kafka-topics.sh --list`

**Spark Machine:**
- [ ] Java 8+ vÃ  Spark 3.5.0+ installed
- [ ] SPARK_HOME set
- [ ] Ports 7077, 8080 opened
- [ ] Run `./start.sh` (sáº½ há»i Kafka IP)
- [ ] Verify: `jps | grep -E "Master|Worker"`
- [ ] Verify: `ls -ld /models /checkpoints`

**Airflow Machine:**
- [ ] Python 3.8+ installed
- [ ] Dependencies: `pip install -r requirements.txt`
- [ ] Ports 8080, 8501 opened
- [ ] Data files accessible (symlink: `airflow_machine/data` â†’ `../../data`)
- [ ] Run `./start.sh` (sáº½ há»i Kafka vÃ  Spark IPs)
- [ ] Config Spark connection trong Airflow UI
- [ ] Verify DAG visible: `airflow dags list`

## ğŸ› Troubleshooting

### Data files not found
- Verify symlink: `ls -la airflow_machine/data/`
- Or copy: `cp ../data/*.csv airflow_machine/data/`
- DAG sáº½ tÃ¬m á»Ÿ: `../../data/`, `../data/`, `data/`

### Spark connection failed
- Check Spark Master: `jps | grep Master`
- Test: `telnet spark-ip 7077`
- Check Airflow connection config (Host, Port, Extra)

### Kafka connection failed
- Check Kafka: `docker ps | grep kafka`
- Test: `telnet kafka-ip 9092`
- Check IP trong scripts Ä‘Ã£ Ä‘Ãºng

### IPs khÃ´ng Ä‘Æ°á»£c update
- Verify `start.sh` Ä‘Ã£ cháº¡y thÃ nh cÃ´ng
- Check: `grep -r "kafka-machine-ip" .`
- Re-run `start.sh` náº¿u cáº§n

### Model path not found
- Check model exists: `ls -lh /models/fraud_detection_v1/`
- Check permissions: `sudo chmod -R 777 /models`
- Verify training task completed

## ğŸ”— Useful Links

- **Kafka UI**: `http://kafka-machine-ip:8080`
- **Spark Web UI**: `http://spark-machine-ip:8080`
- **Airflow UI**: `http://airflow-machine-ip:8080`
- **Streamlit Viewer**: `http://airflow-machine-ip:8501`

## ğŸ‘¥ Authors

Fraud Detection Team

## ğŸ“„ License

Educational Project

---

**Status**: âœ… **READY FOR DEPLOYMENT**

Há»‡ thá»‘ng Ä‘Ã£ Ä‘Æ°á»£c kiá»ƒm tra ká»¹ vÃ  sáºµn sÃ ng Ä‘á»ƒ deploy lÃªn 3 mÃ¡y vÃ  test.
# last
