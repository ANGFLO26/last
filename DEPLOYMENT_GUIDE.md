# HÆ¯á»šNG DáºªN DEPLOY Há»† THá»NG LÃŠN MÃY Má»šI

## ğŸ“‹ Tá»”NG QUAN

HÆ°á»›ng dáº«n nÃ y giÃºp báº¡n deploy há»‡ thá»‘ng Fraud Detection Pipeline lÃªn cÃ¡c mÃ¡y má»›i mÃ  khÃ´ng gáº·p lá»—i vá» paths vÃ  permissions.

## ğŸ” CÃC Váº¤N Äá»€ ÄÃƒ ÄÆ¯á»¢C GIáº¢I QUYáº¾T

### âœ… ÄÃ£ sá»­a:
1. **Hardcoded paths** â†’ Sá»­ dá»¥ng relative paths vÃ  environment variables
2. **File upload** â†’ SparkSubmitOperator tá»± Ä‘á»™ng upload files
3. **Dependencies** â†’ Sá»­ dá»¥ng `files` parameter Ä‘á»ƒ upload utils
4. **IP addresses** â†’ CÃ³ thá»ƒ config qua environment variables
5. **Validation** â†’ ThÃªm task verify scripts tá»“n táº¡i

## ğŸš€ QUY TRÃŒNH DEPLOY

### BÆ°á»›c 1: Clone Repository

TrÃªn **má»—i mÃ¡y**, clone repository:

```bash
git clone <your-repo-url>
cd the_end
```

### BÆ°á»›c 2: Setup Airflow Machine

**TrÃªn Airflow Machine:**

```bash
cd airflow_machine

# 1. Cháº¡y script setup tá»± Ä‘á»™ng
bash setup_deployment.sh
```

Script sáº½:
- Tá»± Ä‘á»™ng detect paths
- Há»i IP addresses cá»§a Kafka vÃ  Spark machines
- Táº¡o file `.env` vá»›i configuration
- Verify scripts vÃ  data files tá»“n táº¡i

**Hoáº·c setup thá»§ cÃ´ng:**

```bash
# Set environment variables
export KAFKA_IP=192.168.1.60
export KAFKA_PORT=9092
export SPARK_IP=192.168.1.134
export SPARK_MASTER_PORT=7077
export SPARK_WEB_UI_PORT=8080

# Paths (tá»± Ä‘á»™ng detect tá»« DAG folder)
export FRAUD_SCRIPTS_DIR=/path/to/airflow_machine/scripts
export FRAUD_UTILS_DIR=/path/to/airflow_machine/utils
export FRAUD_DATA_DIR=/path/to/data

# Spark paths (trÃªn Spark machine)
export SPARK_DATA_DIR=/tmp/fraud_data
export SPARK_MODELS_DIR=/tmp/fraud_models
export SPARK_CHECKPOINTS_DIR=/checkpoints
```

### BÆ°á»›c 3: CÃ i Dependencies

```bash
cd airflow_machine

# Setup virtual environment
bash setup_venv.sh

# Activate venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install Spark client (náº¿u chÆ°a cÃ³)
bash install_spark_client.sh
```

### BÆ°á»›c 4: Verify Spark Connection

```bash
# Test Spark connection
spark-submit --master spark://$SPARK_IP:$SPARK_MASTER_PORT --version
```

Náº¿u lá»—i, kiá»ƒm tra:
- Spark Ä‘Ã£ Ä‘Æ°á»£c cÃ i trÃªn Spark machine
- Network connectivity: `telnet $SPARK_IP $SPARK_MASTER_PORT`
- Firewall rules

### BÆ°á»›c 5: Start Airflow

```bash
cd airflow_machine

# Source environment variables (náº¿u dÃ¹ng .env file)
source .env

# Start Airflow
bash start.sh
```

### BÆ°á»›c 6: Configure Airflow Connection

1. Má»Ÿ Airflow UI: `http://airflow-machine-ip:8080`
2. Login: `admin/admin`
3. **Admin â†’ Connections â†’ Add/Edit:**
   - **Conn Id**: `spark_default`
   - **Conn Type**: `Spark`
   - **Host**: `<SPARK_IP>` (vÃ­ dá»¥: 192.168.1.134)
   - **Port**: `<SPARK_MASTER_PORT>` (vÃ­ dá»¥: 7077)
   - **Extra**: `{"master": "spark://<SPARK_IP>:<SPARK_MASTER_PORT>"}`

### BÆ°á»›c 7: Prepare Spark Machine

**TrÃªn Spark Machine:**

```bash
cd spark_machine

# Táº¡o thÆ° má»¥c cho data vÃ  models
sudo mkdir -p /tmp/fraud_data
sudo mkdir -p /tmp/fraud_models
sudo mkdir -p /checkpoints

# Set permissions (cho phÃ©p táº¥t cáº£ users write)
sudo chmod 777 /tmp/fraud_data
sudo chmod 777 /tmp/fraud_models
sudo chmod 777 /checkpoints

# Copy training data (náº¿u chÆ°a cÃ³)
# Copy train.csv vÃ o /tmp/fraud_data/train.csv

# Start Spark cluster
bash start.sh
```

**Verify Spark:**
```bash
# Check Spark Master
jps | grep Master

# Check Spark Web UI
curl http://localhost:8080
```

### BÆ°á»›c 8: Prepare Kafka Machine

**TrÃªn Kafka Machine:**

```bash
cd kafka_machine

# Start Kafka
bash start.sh

# Verify Kafka
docker ps | grep kafka
```

### BÆ°á»›c 9: Run Pipeline

1. Trong Airflow UI, tÃ¬m DAG `fraud_detection_pipeline`
2. Toggle ON Ä‘á»ƒ enable DAG
3. Click **"Trigger DAG"**
4. Monitor tasks trong Graph View

## ğŸ”§ TROUBLESHOOTING

### Lá»—i: "No such file or directory" khi submit Spark job

**NguyÃªn nhÃ¢n:**
- Script khÃ´ng tá»“n táº¡i táº¡i path chá»‰ Ä‘á»‹nh
- Path khÃ´ng Ä‘Ãºng trÃªn mÃ¡y má»›i

**Giáº£i phÃ¡p:**
1. Cháº¡y `verify_scripts` task Ä‘á»ƒ check
2. Verify paths trong DAG:
   ```python
   # Check trong Airflow UI â†’ Task Logs
   # Hoáº·c cháº¡y:
   python -c "from pathlib import Path; print(Path(__file__).parent)"
   ```
3. Äáº£m báº£o environment variables Ä‘Æ°á»£c set Ä‘Ãºng

### Lá»—i: "Permission denied" trÃªn Spark Worker

**NguyÃªn nhÃ¢n:**
- Spark Worker khÃ´ng cÃ³ quyá»n Ä‘á»c file
- File owner khÃ´ng Ä‘Ãºng

**Giáº£i phÃ¡p:**
```bash
# TrÃªn Spark machine
sudo chmod 755 /tmp/fraud_data
sudo chmod 755 /tmp/fraud_models
sudo chmod 755 /checkpoints

# Hoáº·c set owner
sudo chown -R spark:spark /tmp/fraud_data /tmp/fraud_models /checkpoints
```

### Lá»—i: "ModuleNotFoundError: No module named 'spark_utils'"

**NguyÃªn nhÃ¢n:**
- Dependencies khÃ´ng Ä‘Æ°á»£c upload cÃ¹ng vá»›i main script

**Giáº£i phÃ¡p:**
1. Verify `spark_utils.py` tá»“n táº¡i trong `utils/` folder
2. Check DAG cÃ³ sá»­ dá»¥ng `files` parameter:
   ```python
   files=SPARK_UTILS_FILE if os.path.exists(SPARK_UTILS_FILE) else None,
   ```
3. Náº¿u váº«n lá»—i, cÃ³ thá»ƒ cáº§n Ä‘Ã³ng gÃ³i vÃ o ZIP:
   ```bash
   cd airflow_machine
   zip -r scripts.zip scripts/ utils/
   ```

### Lá»—i: "Connection refused" khi verify services

**NguyÃªn nhÃ¢n:**
- Services chÆ°a start
- IP addresses khÃ´ng Ä‘Ãºng
- Firewall block ports

**Giáº£i phÃ¡p:**
```bash
# Test connectivity
telnet $KAFKA_IP $KAFKA_PORT
telnet $SPARK_IP $SPARK_MASTER_PORT

# Check firewall
sudo ufw status
sudo ufw allow 9092/tcp
sudo ufw allow 7077/tcp
sudo ufw allow 8080/tcp
```

### Lá»—i: "Data files not found"

**NguyÃªn nhÃ¢n:**
- Data files khÃ´ng tá»“n táº¡i táº¡i path chá»‰ Ä‘á»‹nh
- Path khÃ´ng Ä‘Ãºng

**Giáº£i phÃ¡p:**
```bash
# Verify data files
ls -lh $FRAUD_DATA_DIR/stream.csv

# Hoáº·c check trong DAG
# Task verify_data_files sáº½ show path Ä‘ang check
```

## ğŸ“ CHECKLIST DEPLOY

### TrÆ°á»›c khi deploy:

- [ ] Clone repository trÃªn táº¥t cáº£ 3 mÃ¡y
- [ ] Cháº¡y `setup_deployment.sh` trÃªn Airflow machine
- [ ] Set environment variables hoáº·c source `.env` file
- [ ] Verify Spark installation trÃªn Spark machine
- [ ] Verify Kafka installation trÃªn Kafka machine
- [ ] Check network connectivity giá»¯a cÃ¡c machines
- [ ] Verify data files tá»“n táº¡i

### Sau khi deploy:

- [ ] Verify DAG visible trong Airflow UI
- [ ] Test Spark connection tá»« Airflow machine
- [ ] Run `verify_scripts` task thÃ nh cÃ´ng
- [ ] Run `verify_kafka_ready` task thÃ nh cÃ´ng
- [ ] Run `verify_spark_ready` task thÃ nh cÃ´ng
- [ ] Run `verify_data_files` task thÃ nh cÃ´ng
- [ ] Test submit má»™t simple Spark job
- [ ] Run full pipeline

## ğŸ¯ BEST PRACTICES

1. **LuÃ´n sá»­ dá»¥ng environment variables** cho IPs vÃ  paths
2. **Verify scripts tá»“n táº¡i** trÆ°á»›c khi submit job
3. **Set permissions Ä‘Ãºng** trÃªn Spark machine
4. **Test connectivity** trÆ°á»›c khi cháº¡y pipeline
5. **Monitor logs** trong Airflow UI Ä‘á»ƒ debug

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- [DEPLOYMENT_ANALYSIS.md](./DEPLOYMENT_ANALYSIS.md) - PhÃ¢n tÃ­ch chi tiáº¿t cÃ¡c váº¥n Ä‘á»
- [README.md](./README.md) - TÃ i liá»‡u chÃ­nh cá»§a dá»± Ã¡n
- [TESTING_GUIDE.md](./airflow_machine/TESTING_GUIDE.md) - HÆ°á»›ng dáº«n testing

