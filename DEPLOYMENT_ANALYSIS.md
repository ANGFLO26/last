# PHÃ‚N TÃCH Há»† THá»NG VÃ€ CÃC Váº¤N Äá»€ KHI DEPLOY

## ğŸ” Tá»”NG QUAN Váº¤N Äá»€

Khi deploy há»‡ thá»‘ng lÃªn cÃ¡c mÃ¡y khÃ¡c, gáº·p lá»—i: **Spark Worker khÃ´ng thá»ƒ truy cáº­p file Python scripts** (`train_model.py`, `streaming_inference.py`).

## ğŸ“‹ PHÃ‚N TÃCH CHI TIáº¾T CÃC Váº¤N Äá»€

### 1. âŒ HARDCODED PATHS (ÄÆ°á»ng dáº«n cá»©ng)

**Váº¥n Ä‘á»:**
- DAG sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i: `/home/phanvantai/Documents/four_years/bigdata/the_end/...`
- Chá»‰ Ä‘Ãºng trÃªn mÃ¡y hiá»‡n táº¡i
- Khi deploy lÃªn mÃ¡y khÃ¡c, user khÃ¡c, path khÃ¡c â†’ **Lá»–I**

**Vá»‹ trÃ­:**
```python
# airflow_machine/dags/fraud_detection_pipeline.py
application='/home/phanvantai/Documents/four_years/bigdata/the_end/airflow_machine/scripts/train_model.py'
```

**áº¢nh hÆ°á»Ÿng:**
- SparkSubmitOperator khÃ´ng tÃ¬m tháº¥y file
- Airflow khÃ´ng thá»ƒ submit job

---

### 2. âŒ SPARK WORKER PERMISSIONS (Quyá»n truy cáº­p)

**Váº¥n Ä‘á»:**
- SparkSubmitOperator upload file tá»« Airflow machine lÃªn Spark cluster
- File Ä‘Æ°á»£c upload vÃ o thÆ° má»¥c táº¡m cá»§a Spark (thÆ°á»ng lÃ  `/tmp/spark-*`)
- Spark Worker cháº¡y dÆ°á»›i user khÃ¡c (cÃ³ thá»ƒ lÃ  `spark` user hoáº·c user khÃ¡c)
- Worker khÃ´ng cÃ³ quyá»n truy cáº­p vÃ o:
  - Home directory cá»§a user Airflow
  - ThÆ° má»¥c táº¡m náº¿u permissions khÃ´ng Ä‘Ãºng

**CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng:**
1. Airflow (user: `phanvantai`) cháº¡y `spark-submit`
2. Spark Master nháº­n job vÃ  phÃ¢n phÃ¡t cho Worker
3. Worker (user: `spark` hoáº·c user khÃ¡c) cáº§n truy cáº­p file
4. **Lá»–I**: Permission denied hoáº·c File not found

---

### 3. âŒ PYTHON DEPENDENCIES (Phá»¥ thuá»™c Python)

**Váº¥n Ä‘á»:**
- Script `verify_streaming_job.py` import tá»« `utils/spark_utils.py`:
  ```python
  from spark_utils import verify_spark_job_running
  ```
- Khi SparkSubmitOperator upload file, nÃ³ chá»‰ upload file chÃ­nh
- **KHÃ”NG upload** cÃ¡c file dependencies trong `utils/`
- Spark Worker khÃ´ng tÃ¬m tháº¥y module `spark_utils`

**Vá»‹ trÃ­:**
```python
# airflow_machine/scripts/verify_streaming_job.py
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'utils'))
from spark_utils import verify_spark_job_running
```

**áº¢nh hÆ°á»Ÿng:**
- ImportError khi cháº¡y script
- Job fail ngay khi start

---

### 4. âŒ SPARK INSTALLATION PATH (ÄÆ°á»ng dáº«n cÃ i Ä‘áº·t Spark)

**Váº¥n Ä‘á»:**
- Script giáº£ Ä‘á»‹nh Spark á»Ÿ `/opt/spark`
- TrÃªn cÃ¡c mÃ¡y khÃ¡c cÃ³ thá»ƒ:
  - Spark á»Ÿ vá»‹ trÃ­ khÃ¡c (`/usr/local/spark`, `~/spark`, etc.)
  - KhÃ´ng cÃ³ Spark installed
  - SPARK_HOME khÃ´ng Ä‘Æ°á»£c set

**Vá»‹ trÃ­:**
```bash
# spark_machine/start_spark_cluster.sh
SPARK_HOME=${SPARK_HOME:-/opt/spark}
```

**áº¢nh hÆ°á»Ÿng:**
- KhÃ´ng start Ä‘Æ°á»£c Spark cluster
- SparkSubmitOperator khÃ´ng tÃ¬m tháº¥y `spark-submit`

---

### 5. âŒ DATA FILE PATHS (ÄÆ°á»ng dáº«n file dá»¯ liá»‡u)

**Váº¥n Ä‘á»:**
- Training data: `/tmp/fraud_data/train.csv` (trÃªn Spark machine)
- Stream data: `/home/phanvantai/.../data/stream.csv` (trÃªn Airflow machine)
- Hardcoded paths khÃ´ng flexible

**Vá»‹ trÃ­:**
```python
# DAG
application_args=[
    '--input', '/tmp/fraud_data/train.csv',
    '--output', '/tmp/fraud_models/fraud_detection_v1',
]

# verify_data_files task
DATA_DIR="/home/phanvantai/Documents/four_years/bigdata/the_end/data"
```

**áº¢nh hÆ°á»Ÿng:**
- File khÃ´ng tá»“n táº¡i trÃªn mÃ¡y khÃ¡c
- Job fail khi khÃ´ng tÃ¬m tháº¥y data

---

### 6. âŒ NETWORK CONFIGURATION (Cáº¥u hÃ¬nh máº¡ng)

**Váº¥n Ä‘á»:**
- IP addresses hardcoded: `192.168.1.60`, `192.168.1.134`
- KhÃ´ng tá»± Ä‘á»™ng detect IP
- KhÃ´ng cÃ³ cÆ¡ cháº¿ fallback

**Vá»‹ trÃ­:**
```python
# DAG
'timeout 5 bash -c \'cat < /dev/null > /dev/tcp/192.168.1.60/9092\''
'spark.master': 'spark://192.168.1.134:7077'
```

**áº¢nh hÆ°á»Ÿng:**
- KhÃ´ng connect Ä‘Æ°á»£c Ä‘áº¿n services
- Verify tasks fail

---

### 7. âŒ FILE UPLOAD MECHANISM (CÆ¡ cháº¿ upload file)

**Váº¥n Ä‘á»:**
- SparkSubmitOperator upload file qua network
- File Ä‘Æ°á»£c copy vÃ o thÆ° má»¥c táº¡m trÃªn Spark Master
- Spark Master phÃ¢n phÃ¡t file cho Workers
- **NHÆ¯NG**: Náº¿u file lá»›n hoáº·c network cháº­m â†’ timeout
- Náº¿u permissions khÃ´ng Ä‘Ãºng â†’ Worker khÃ´ng Ä‘á»c Ä‘Æ°á»£c

**CÆ¡ cháº¿:**
```
Airflow Machine (file: train_model.py)
    â†“ (upload via spark-submit)
Spark Master (/tmp/spark-xxx/train_model.py)
    â†“ (distribute to workers)
Spark Workers (/tmp/spark-xxx/train_model.py)
```

**Váº¥n Ä‘á» tiá»m áº©n:**
- File upload timeout
- Worker khÃ´ng cÃ³ quyá»n Ä‘á»c
- File bá»‹ corrupt trong quÃ¡ trÃ¬nh transfer

---

## âœ… GIáº¢I PHÃP Äá»€ XUáº¤T

### Giáº£i phÃ¡p 1: Sá»­ dá»¥ng Relative Paths + Environment Variables

**Thay Ä‘á»•i:**
- Sá»­ dá»¥ng `{{ dag.folder }}` hoáº·c `os.path.dirname(__file__)` Ä‘á»ƒ láº¥y path tÆ°Æ¡ng Ä‘á»‘i
- Sá»­ dá»¥ng environment variables cho cÃ¡c paths quan trá»ng

**Code:**
```python
import os
from pathlib import Path

# Láº¥y path tÆ°Æ¡ng Ä‘á»‘i tá»« DAG folder
DAG_DIR = Path(__file__).parent
SCRIPTS_DIR = DAG_DIR.parent / "scripts"
DATA_DIR = os.getenv("FRAUD_DATA_DIR", str(DAG_DIR.parent.parent / "data"))

application=str(SCRIPTS_DIR / "train_model.py")
```

---

### Giáº£i phÃ¡p 2: Copy Files vÃ o Shared Location trÃªn Spark Machine

**Thay Ä‘á»•i:**
- TrÆ°á»›c khi submit job, copy files vÃ o thÆ° má»¥c shared trÃªn Spark machine
- Sá»­ dá»¥ng thÆ° má»¥c cÃ³ quyá»n truy cáº­p cÃ´ng khai: `/tmp/spark_scripts/` hoáº·c `/opt/spark_scripts/`

**Code:**
```python
# Task má»›i: prepare_scripts
prepare_scripts = BashOperator(
    task_id='prepare_scripts',
    bash_command=f"""
        # Copy scripts lÃªn Spark machine
        scp {SCRIPTS_DIR}/*.py spark-machine:/tmp/spark_scripts/
        # Hoáº·c dÃ¹ng rsync qua SSH
    """,
)
```

**Váº¥n Ä‘á»:** Cáº§n SSH setup giá»¯a Airflow vÃ  Spark machines

---

### Giáº£i phÃ¡p 3: Sá»­ dá»¥ng PyFiles Ä‘á»ƒ Upload Dependencies

**Thay Ä‘á»•i:**
- Sá»­ dá»¥ng `py_files` parameter cá»§a SparkSubmitOperator Ä‘á»ƒ upload cÃ¡c file dependencies
- ÄÃ³ng gÃ³i utils vÃ o má»™t package

**Code:**
```python
train_model = SparkSubmitOperator(
    task_id='train_model',
    application=str(SCRIPTS_DIR / "train_model.py"),
    py_files=[
        str(DAG_DIR.parent / "utils" / "spark_utils.py"),
    ],
    # ...
)
```

---

### Giáº£i phÃ¡p 4: Sá»­ dá»¥ng Spark Archives (ZIP)

**Thay Ä‘á»•i:**
- ÄÃ³ng gÃ³i toÃ n bá»™ scripts vÃ  dependencies vÃ o má»™t ZIP file
- Upload ZIP file lÃªn Spark cluster
- Spark tá»± Ä‘á»™ng extract vÃ  thÃªm vÃ o PYTHONPATH

**Code:**
```python
# Táº¡o ZIP file chá»©a scripts vÃ  utils
# Trong DAG:
train_model = SparkSubmitOperator(
    task_id='train_model',
    application=str(SCRIPTS_DIR / "train_model.py"),
    archives=[str(SCRIPTS_DIR.parent / "scripts_archive.zip")],
    # ...
)
```

---

### Giáº£i phÃ¡p 5: Sá»­ dá»¥ng Shared Filesystem (NFS/S3)

**Thay Ä‘á»•i:**
- Mount shared filesystem (NFS) trÃªn cáº£ 3 mÃ¡y
- LÆ°u scripts vÃ  data trÃªn shared filesystem
- Táº¥t cáº£ machines truy cáº­p cÃ¹ng má»™t path

**VÃ­ dá»¥:**
```
/shared/fraud_detection/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_model.py
â”‚   â””â”€â”€ streaming_inference.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ stream.csv
â””â”€â”€ models/
```

**Code:**
```python
SHARED_DIR = "/shared/fraud_detection"
application=f"{SHARED_DIR}/scripts/train_model.py"
```

---

### Giáº£i phÃ¡p 6: Sá»­ dá»¥ng Docker Ä‘á»ƒ Äáº£m báº£o Consistency

**Thay Ä‘á»•i:**
- ÄÃ³ng gÃ³i scripts vÃ o Docker image
- Spark Workers cháº¡y trong Docker containers
- Äáº£m báº£o mÃ´i trÆ°á»ng giá»‘ng nhau trÃªn táº¥t cáº£ machines

**Váº¥n Ä‘á»:** Phá»©c táº¡p hÆ¡n, cáº§n setup Docker trÃªn Spark cluster

---

## ğŸ¯ GIáº¢I PHÃP ÄÆ¯á»¢C KHUYáº¾N NGHá»Š

### Káº¿t há»£p Giáº£i phÃ¡p 1 + 3 + 4:

1. **Relative Paths**: Sá»­ dá»¥ng relative paths thay vÃ¬ hardcoded
2. **PyFiles**: Upload dependencies qua `py_files`
3. **Environment Variables**: Sá»­ dá»¥ng env vars cho IPs vÃ  paths
4. **Validation**: ThÃªm tasks Ä‘á»ƒ verify files tá»“n táº¡i trÆ°á»›c khi submit

### Implementation Plan:

1. âœ… Sá»­a DAG Ä‘á»ƒ sá»­ dá»¥ng relative paths
2. âœ… ThÃªm `py_files` cho SparkSubmitOperator
3. âœ… Táº¡o task Ä‘á»ƒ verify scripts tá»“n táº¡i
4. âœ… Sá»­ dá»¥ng environment variables cho IPs
5. âœ… Táº¡o script setup Ä‘á»ƒ copy files vÃ o shared location (optional)

---

## ğŸ“ CHECKLIST KHI DEPLOY

### TrÆ°á»›c khi deploy:

- [ ] Kiá»ƒm tra Spark installation path trÃªn Spark machine
- [ ] Kiá»ƒm tra SPARK_HOME Ä‘Æ°á»£c set Ä‘Ãºng
- [ ] Kiá»ƒm tra permissions trÃªn Spark machine (`/tmp`, `/opt/spark_scripts`)
- [ ] Kiá»ƒm tra network connectivity giá»¯a cÃ¡c machines
- [ ] Kiá»ƒm tra Python version trÃªn Spark Workers (pháº£i >= 3.8)
- [ ] Kiá»ƒm tra PySpark Ä‘Æ°á»£c cÃ i trÃªn Spark Workers

### Sau khi deploy:

- [ ] Test SparkSubmitOperator vá»›i simple job
- [ ] Verify files Ä‘Æ°á»£c upload Ä‘Ãºng
- [ ] Check Spark Worker logs Ä‘á»ƒ xem cÃ³ lá»—i permissions khÃ´ng
- [ ] Test vá»›i actual scripts (train_model.py, streaming_inference.py)

---

## ğŸ”§ TROUBLESHOOTING

### Lá»—i: "No such file or directory"

**NguyÃªn nhÃ¢n:**
- File khÃ´ng tá»“n táº¡i táº¡i path chá»‰ Ä‘á»‹nh
- Path khÃ´ng Ä‘Ãºng trÃªn mÃ¡y khÃ¡c

**Giáº£i phÃ¡p:**
- Sá»­ dá»¥ng relative paths
- Verify file tá»“n táº¡i trÆ°á»›c khi submit

### Lá»—i: "Permission denied"

**NguyÃªn nhÃ¢n:**
- Spark Worker khÃ´ng cÃ³ quyá»n Ä‘á»c file
- File owner khÃ´ng Ä‘Ãºng

**Giáº£i phÃ¡p:**
- Copy file vÃ o thÆ° má»¥c cÃ³ quyá»n cÃ´ng khai (`/tmp/spark_scripts/`)
- Set permissions: `chmod 755 /tmp/spark_scripts/*.py`

### Lá»—i: "ModuleNotFoundError: No module named 'spark_utils'"

**NguyÃªn nhÃ¢n:**
- Dependencies khÃ´ng Ä‘Æ°á»£c upload cÃ¹ng vá»›i main script

**Giáº£i phÃ¡p:**
- Sá»­ dá»¥ng `py_files` parameter
- Hoáº·c Ä‘Ã³ng gÃ³i vÃ o ZIP file

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

- [SparkSubmitOperator Documentation](https://airflow.apache.org/docs/apache-airflow-providers-apache-spark/stable/operators/spark-submit.html)
- [Spark Application Submission](https://spark.apache.org/docs/latest/submitting-applications.html)
- [Spark Python Dependencies](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)

